#====================================================================
    PALM Method with Projection

    Solve problem,
    (ùí´) min_{x,Œ∏ ‚àà ùíü} œÅ(Ax - y;Œ∏) + m‚ãÖlog[c(Œ∏)]
====================================================================#
using ForwardDiff
using Printf

##### PALMProj Parameters #####
mutable struct PALMProj_params
    tol::Float64
    iter_max::Int64
    nt_tol::Float64
    nt_iter_max::Int64
    convergent_history::Array{Float64,1}
end

##### PALM with Projection Solver #####
function PALMProj(primal::PLQ_Primal, params::PALMProj_params)

    # primal variables
    A = primal.A; y = primal.y;
    x = primal.x; Œ∏ = primal.Œ∏;
    m,n = size(A)

    # PALM projection parameters
    tol = params.tol;
    iter_max = params.iter_max;
    nt_tol = params.nt_tol;
    nt_iter_max = params.nt_iter_max;
    convergent_history = params.convergent_history

    # PALM iterations
    noi = 0;
    err = 1.0;
    r   = y - A*x;
    convergent_history[1] = primal.œÅ(r,Œ∏) + m*primal.lognc(Œ∏)
    while err ‚â• tol
        # calculate Lipschitz constant
        c  = primal.L‚ÇÅ(Œ∏,A)
        # update x
        f(r) = primal.œÅ(r,Œ∏)
        g  = transpose(A)*ForwardDiff.gradient(f,r) # gradient
        xn = x + g/c                    # gradient descent step
        r  = y - A*xn                       # update residual
        # update Œ∏
        # Œ∏n = copy(Œ∏)
        Œ∏n = projŒ∏(r,Œ∏,primal, nt_tol, nt_iter_max)
        # update convergent history
        noi += 1
        # err = vecnorm(Œ∏n-Œ∏) + vecnorm(xn-x)
        err = vecnorm(g)
        copy!(x,xn)
        copy!(Œ∏,Œ∏n)
        obj = primal.œÅ(r,Œ∏) + m*primal.lognc(Œ∏)
        @printf("iter %3d, obj %1.5e, err %1.5e\n",noi,obj,err)
        convergent_history[noi+1] = obj
        if noi ‚â• iter_max
            println("Reach Maximum Iterations!")
            break
        end
    end
    params.convergent_history = convergent_history[1:noi+1]
end

##### Project Fuction #####
function projŒ∏(r, Œ∏, primal, nt_tol, nt_iter_max)
    m,n = size(primal.A)
    f(Œ∏) = primal.œÅ(r,Œ∏)
    S = primal.S
    s = primal.s
    Œº = 1.0
    # newton solver for interior point method
    H  = ForwardDiff.hessian(f,Œ∏)  + m*ForwardDiff.hessian(primal.lognc,Œ∏)
    g  = ForwardDiff.gradient(f,Œ∏) + m*ForwardDiff.gradient(primal.lognc,Œ∏)
    # KKT system
    d   = s - S'*Œ∏; k = length(d); q   = ones(k);
    FŒº  = [diagm(d)*q - Œº*ones(k); g + S*q]
    ‚àáFŒº = [diagm(d) -diagm(q)*S'; S H]
    idq = 1:k; idŒ∏ = k+1:k+length(Œ∏);
    z   = [q; Œ∏]
    # start iteration
    nt_noi = 0
    nt_err = vecnorm(g)
    while nt_err ‚â• nt_tol
        # calculate direction
        p  = ‚àáFŒº \ FŒº
        # line search
        Œ∑  = 1.0
        dd = -S'*p[idŒ∏]
        for i = 1:k
            dd[i] ‚â§ 0.0 && continue
            Œ∑ = min(Œ∑,d[i]/dd[i])
        end
        Œ∑ -= 1e-2*Œº
        z -= Œ∑*p
        # update variable gradient hessian
        Œ∏  = z[idŒ∏]; q = z[idq];
        H  = ForwardDiff.hessian(f,Œ∏)  + m*ForwardDiff.hessian(primal.lognc,Œ∏)
        g  = ForwardDiff.gradient(f,Œ∏) + m*ForwardDiff.gradient(primal.lognc,Œ∏)
        d  = s - S'*Œ∏
        FŒº  = [diagm(d)*q - Œº*ones(k); g + S*q]
        ‚àáFŒº = [diagm(d) -diagm(q)*S'; S H]
        # update convergence information
        nt_noi += 1
        nt_err = vecnorm(FŒº)
        Œº  = 0.1*sum(d.*q)/k
        if nt_noi ‚â• nt_iter_max
            println("Newton Method Reach Maximum Iterations!")
            break
        end
        # obj = f(Œ∏) + m*primal.lognc(Œ∏)
        # @printf("iter %2d, obj %1.5e, err %1.5e\n", nt_noi, obj, nt_err)
    end
    return Œ∏
end